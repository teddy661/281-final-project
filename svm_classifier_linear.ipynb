{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "try:\n",
    "    from sklearnex import patch_sklearn\n",
    "except ImportError:\n",
    "    %pip install scikit-learn-intelex\n",
    "    from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from classification_data_loader import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df, test_df, meta_df = load_raw_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_df = get_sampled_data(raw_train_df)\n",
    "train_df, validation_df = split_data(sampled_train_df)\n",
    "del sampled_train_df, raw_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LBP\n",
    "LBP_scalar = StandardScaler()\n",
    "\n",
    "X_train_LBP, X_test_LBP, X_validation_LBP = get_lbp_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "X_train_LBP = LBP_scalar.fit_transform(X_train_LBP)\n",
    "X_test_LBP = LBP_scalar.transform(X_test_LBP)\n",
    "X_validation_LBP = LBP_scalar.transform(X_validation_LBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HSV Features\n",
    "Hue_scaler = StandardScaler()\n",
    "Sat_Scaler = StandardScaler()\n",
    "Val_Scaler = StandardScaler()\n",
    "\n",
    "X_train_Hue, X_test_Hue, X_validation_Hue = get_hue_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "(\n",
    "    X_train_Saturation,\n",
    "    X_test_Saturation,\n",
    "    X_validation_Saturation,\n",
    ") = get_saturation_features(train_df, test_df, validation_df)\n",
    "X_train_Value, X_test_Value, X_validation_Value = get_value_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "\n",
    "X_train_Hue = Hue_scaler.fit_transform(X_train_Hue)\n",
    "X_train_Saturation = Sat_Scaler.fit_transform(X_train_Saturation)\n",
    "X_train_Value = Val_Scaler.fit_transform(X_train_Value)\n",
    "\n",
    "X_test_Hue = Hue_scaler.transform(X_test_Hue)\n",
    "X_test_Saturation = Sat_Scaler.transform(X_test_Saturation)\n",
    "X_test_Value = Val_Scaler.transform(X_test_Value)\n",
    "\n",
    "X_validation_Hue = Hue_scaler.transform(X_validation_Hue)\n",
    "X_validation_Saturation = Sat_Scaler.transform(X_validation_Saturation)\n",
    "X_validation_Value = Val_Scaler.transform(X_validation_Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Template Fratures\n",
    "Template_scalar = StandardScaler()\n",
    "\n",
    "X_train_Template, X_test_Template, X_vaidation_Template = get_template_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "\n",
    "X_train_Template = Template_scalar.fit_transform(X_train_Template)\n",
    "X_test_Template = Template_scalar.transform(X_test_Template)\n",
    "X_vaidation_Template = Template_scalar.transform(X_vaidation_Template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HOG Features\n",
    "HOG_scaler = StandardScaler()\n",
    "\n",
    "X_train_HOG, X_test_HOG, X_validation_HOG = get_hog_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "\n",
    "X_train_HOG = HOG_scaler.fit_transform(X_train_HOG)\n",
    "X_test_HOG = HOG_scaler.transform(X_test_HOG)\n",
    "X_validation_HOG = HOG_scaler.transform(X_validation_HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VGG16 Features\n",
    "VGG16_scaler = StandardScaler()\n",
    "\n",
    "X_train_VGG16, X_test_VGG16, X_validation_VGG16 = get_vgg16_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "X_train_VGG16 = VGG16_scaler.fit_transform(X_train_VGG16)\n",
    "X_test_VGG = VGG16_scaler.transform(X_test_VGG16)\n",
    "X_validation_VGG = VGG16_scaler.transform(X_validation_VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESNET101 Features\n",
    "RESNET101_scaler = StandardScaler()\n",
    "\n",
    "X_train_RESNET101, X_test_RESNET101, X_validation_RESNET101 = get_resnet101_features(\n",
    "    train_df, test_df, validation_df\n",
    ")\n",
    "X_train_RESNET101 = RESNET101_scaler.fit_transform(X_train_RESNET101)\n",
    "X_test_RESNET101 = RESNET101_scaler.transform(X_test_RESNET101)\n",
    "X_validation_RESNET101 = RESNET101_scaler.transform(X_validation_RESNET101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Construct the Training, Test and Validation Datasets\n",
    "\n",
    "\n",
    "X_train = np.concatenate(\n",
    "    (X_train_Hue, X_train_Saturation, X_train_HOG, X_train_Template, X_train_LBP),\n",
    "    axis=1,\n",
    ")\n",
    "X_test = np.concatenate(\n",
    "    (X_test_Hue, X_test_Saturation, X_test_HOG, X_test_Template, X_test_LBP), axis=1\n",
    ")\n",
    "X_validation = np.concatenate(\n",
    "    (\n",
    "        X_validation_Hue,\n",
    "        X_validation_Saturation,\n",
    "        X_validation_HOG,\n",
    "        X_vaidation_Template,\n",
    "        X_validation_LBP,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.concatenate((X_train_HOG,), axis=1)\n",
    "# X_test = np.concatenate((X_test_HOG,), axis=1)\n",
    "# X_validation = np.concatenate((X_validation_HOG,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[\"ClassId\"].to_numpy()\n",
    "y_test = test_df[\"ClassId\"].to_numpy()\n",
    "y_validation = validation_df[\"ClassId\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Kernels seem to work very well for our data\n",
    "joblib_file = Path(\"best_overall_svc_classifier.joblib\")\n",
    "if not joblib_file.exists():\n",
    "    # These parameters wer obtained with GridSearchCV which is currently disabled since it takes over an hour to run\n",
    "    # We will use a linear kernel, because it seems to work very well for our data\n",
    "    # We will use a C value of 0.01, because it seems to work very well for our data\n",
    "    # We will use a one-vs-one decision function, because it seems to work very well for our data\n",
    "    # We will use a probability function, because it seems to work very well for our data\n",
    "    # We will use a random state of 42, so we can reproduce the results\n",
    "    svc_model = make_pipeline(\n",
    "        # StandardScaler(),\n",
    "        SVC(\n",
    "            kernel=\"linear\",\n",
    "            C=0.01,\n",
    "            decision_function_shape=\"ovo\",\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "        ),\n",
    "    )\n",
    "    svc_model.fit(X_train, y_train)\n",
    "else:\n",
    "    svc_model = joblib.load(joblib_file)\n",
    "    print(\"Loaded SVC Model from file\")\n",
    "hyperparameters = svc_model.get_params()\n",
    "print(\"Hyperparameters using get_params():\")\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Running the Hyperparameter search takes 4 hours. We did this multiple times. Leave it off for now\n",
    "## This needs over 128GB of RAM to run due to n_jobs=4\n",
    "##\n",
    "do_param_search = False\n",
    "if do_param_search:\n",
    "    # Previous Runs say linear and 0.1 are the best\n",
    "    pipeline = make_pipeline(StandardScaler(), SVC(random_state=42))\n",
    "    param_grid = {\n",
    "        \"svc__kernel\": [\"linear\", \"rbf\"],\n",
    "        \"svc__C\": [0.005, 0.01, 0.05, 0.1],\n",
    "        \"svc__gamma\": [\"scale\", \"auto\"],\n",
    "        \"svc__decision_function_shape\": [\"ovo\", \"ovr\"],\n",
    "        \"svc__probability\": [True, False],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, n_jobs=4, cv=5, scoring=\"accuracy\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "    joblib.dump(best_model, \"best_overall_svc_classifier.joblib\")\n",
    "else:\n",
    "    print(\"Skipped Hyperparameter Search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"best_model\" in locals() or \"best_model\" in globals():\n",
    "    svc_model = best_model\n",
    "y_pred = svc_model.predict(X_validation)\n",
    "accuracy = accuracy_score(y_validation, y_pred)\n",
    "print(\"\\nAccuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = (\n",
    "    confusion_matrix_result.astype(\"float\")\n",
    "    / confusion_matrix_result.sum(axis=1)[:, np.newaxis]\n",
    "    * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "class_labels = [f\"Class {i}\" for i in range(43)]\n",
    "\n",
    "\n",
    "# Create a color map to represent the count values\n",
    "color_map = ListedColormap(sns.color_palette(\"inferno\"))\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(\n",
    "    cm_percent,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=color_map,\n",
    "    linewidths=0.5,\n",
    "    square=True,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    annot_kws={\"size\": 10},\n",
    ")\n",
    "\n",
    "# Customize labels and title\n",
    "plt.xlabel(\"Predicted\", fontsize=12)\n",
    "plt.ylabel(\"Actual\", fontsize=12)\n",
    "plt.title(\"Confusion Matrix\", fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "\n",
    "# Adjust layout to prevent cutoff of labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "def display_classification_report(report):\n",
    "    report_html = HTML(f\"<pre>{report}</pre>\")\n",
    "    display(report_html)\n",
    "\n",
    "\n",
    "# Display the formatted classification report\n",
    "display_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_confused = 10  # Change this to display more or less classes\n",
    "flat_cm_percentage = cm_percent.flatten()\n",
    "# Filter out cases where row index equals column index\n",
    "non_equal_indices = np.where(\n",
    "    np.arange(flat_cm_percentage.size) // cm_percent.shape[1]\n",
    "    != np.arange(flat_cm_percentage.size) % cm_percent.shape[1]\n",
    ")\n",
    "\n",
    "# Get the indices of the top 4 most confused items excluding equal indices\n",
    "top_confused_indices = np.argsort(flat_cm_percentage[non_equal_indices])[::-1][\n",
    "    :num_classes_confused\n",
    "]\n",
    "\n",
    "# Reshape the indices to get corresponding row and column indices\n",
    "row_indices, col_indices = np.unravel_index(\n",
    "    non_equal_indices[0][top_confused_indices], cm_percent.shape\n",
    ")\n",
    "\n",
    "# Display the top 4 most confused items\n",
    "top_confused_items = list(\n",
    "    zip(\n",
    "        row_indices,\n",
    "        col_indices,\n",
    "        flat_cm_percentage[non_equal_indices][top_confused_indices],\n",
    "    )\n",
    ")\n",
    "print(f\"\\tTop {num_classes_confused} Most Confused Classes:\")\n",
    "print(45 * \"-\")\n",
    "for row, col, percentage in top_confused_items:\n",
    "    print(f\"True Class: {row:2d}\\tPredicted Class {col:2d}:   {percentage:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (row, col, _) in enumerate(top_confused_items):\n",
    "    predicted_image = (\n",
    "        np.load(BytesIO(meta_df.filter(meta_df[\"ClassId\"] == col)[\"Meta_Image\"][0]))\n",
    "        * 255.0\n",
    "    ).astype(np.uint8)\n",
    "    confused_image = (\n",
    "        np.load(BytesIO(test_df.filter(test_df[\"ClassId\"] == row)[\"Image\"][0])) * 255.0\n",
    "    ).astype(np.uint8)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    fig.suptitle(f\"True Label: {row} Predicted Label {col}\", fontsize=18)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.set_title(\"Confused Class \" + str(row))\n",
    "    ax.imshow(confused_image)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.set_title(\"Predicted ClassId \" + str(col))\n",
    "    ax.imshow(predicted_image)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    if i + 1 >= 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "281-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
