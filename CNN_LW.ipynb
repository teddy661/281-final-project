{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pulls the data from the Features.parquet file and is used to validate that we preseve the data accross saves to parquet format. Data should compare with the develop_simple_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "from feature_utils import normalize_histogram\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Conv1D, MaxPooling1D, Dropout, BatchNormalization, LeakyReLU, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file, this takes a while. Leave it here\n",
    "features_file = Path(\"data/train_features.parquet\")\n",
    "if not features_file.exists():\n",
    "    print(\"No features file found. Please run the create_features_table first\")\n",
    "    exit(1)\n",
    "\n",
    "meta_file = Path(\"data/meta_full.parquet\")\n",
    "if not meta_file.exists():\n",
    "    print(\"No features file found. Please run the create_features_table first\")\n",
    "    exit(1)\n",
    "\n",
    "dft = pl.read_parquet(features_file, memory_map=True)\n",
    "dfm = pl.read_parquet(meta_file, memory_map=True)\n",
    "dft = dft.join(dfm, on=\"ClassId\")\n",
    "# del dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 19)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ClassId</th><th>Width</th><th>Height</th><th>Image</th><th>Resolution</th><th>Hue_Hist</th><th>Saturation_Hist</th><th>Value_Hist</th><th>LBP_Image</th><th>LBP_Hist</th><th>HOG_Features</th><th>HOG_Image</th><th>SIFT_Features</th><th>Path</th><th>ShapeId</th><th>ColorId</th><th>SignId</th><th>Description</th><th>Meta_Image</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>binary</td><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>binary</td><td>list[i64]</td><td>list[f32]</td><td>binary</td><td>binary</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>binary</td></tr></thead><tbody><tr><td>20</td><td>64</td><td>64</td><td>[binary data]</td><td>4096</td><td>[247, 0, … 0]</td><td>[30, 0, … 0]</td><td>[0, 0, … 0]</td><td>[binary data]</td><td>[127, 95, … 760]</td><td>[0.318327, 0.160083, … 0.28708]</td><td>[binary data]</td><td>[binary data]</td><td>&quot;C:\\Users\\lisaw…</td><td>0</td><td>0</td><td>&quot;1.1&quot;</td><td>&quot;Right curve&quot;</td><td>[binary data]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 19)\n",
       "┌─────────┬───────┬────────┬───────────────┬───┬─────────┬────────┬─────────────┬───────────────┐\n",
       "│ ClassId ┆ Width ┆ Height ┆ Image         ┆ … ┆ ColorId ┆ SignId ┆ Description ┆ Meta_Image    │\n",
       "│ ---     ┆ ---   ┆ ---    ┆ ---           ┆   ┆ ---     ┆ ---    ┆ ---         ┆ ---           │\n",
       "│ i64     ┆ i64   ┆ i64    ┆ binary        ┆   ┆ i64     ┆ str    ┆ str         ┆ binary        │\n",
       "╞═════════╪═══════╪════════╪═══════════════╪═══╪═════════╪════════╪═════════════╪═══════════════╡\n",
       "│ 20      ┆ 64    ┆ 64     ┆ [binary data] ┆ … ┆ 0       ┆ 1.1    ┆ Right curve ┆ [binary data] │\n",
       "└─────────┴───────┴────────┴───────────────┴───┴─────────┴────────┴─────────────┴───────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = 200\n",
    "random_seed = 42\n",
    "train_sampled_df = pl.concat([x.sample(samples_per_class, with_replacement=False, seed=random_seed) for x in dft.partition_by(\"ClassId\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ClassId',\n",
       " 'Width',\n",
       " 'Height',\n",
       " 'Image',\n",
       " 'Resolution',\n",
       " 'Hue_Hist',\n",
       " 'Saturation_Hist',\n",
       " 'Value_Hist',\n",
       " 'LBP_Image',\n",
       " 'LBP_Hist',\n",
       " 'HOG_Features',\n",
       " 'HOG_Image',\n",
       " 'SIFT_Features',\n",
       " 'Path',\n",
       " 'ShapeId',\n",
       " 'ColorId',\n",
       " 'SignId',\n",
       " 'Description',\n",
       " 'Meta_Image']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sampled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ClassId</th><th>Hue_Hist</th><th>Saturation_Hist</th><th>Value_Hist</th><th>LBP_Hist</th><th>HOG_Features</th></tr><tr><td>i64</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>list[f32]</td></tr></thead><tbody><tr><td>20</td><td>[972, 151, … 16]</td><td>[247, 0, … 0]</td><td>[0, 0, … 0]</td><td>[80, 151, … 994]</td><td>[0.25923, 0.184951, … 0.0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 6)\n",
       "┌─────────┬──────────────────┬─────────────────┬─────────────┬──────────────────┬──────────────────┐\n",
       "│ ClassId ┆ Hue_Hist         ┆ Saturation_Hist ┆ Value_Hist  ┆ LBP_Hist         ┆ HOG_Features     │\n",
       "│ ---     ┆ ---              ┆ ---             ┆ ---         ┆ ---              ┆ ---              │\n",
       "│ i64     ┆ list[i64]        ┆ list[i64]       ┆ list[i64]   ┆ list[i64]        ┆ list[f32]        │\n",
       "╞═════════╪══════════════════╪═════════════════╪═════════════╪══════════════════╪══════════════════╡\n",
       "│ 20      ┆ [972, 151, … 16] ┆ [247, 0, … 0]   ┆ [0, 0, … 0] ┆ [80, 151, … 994] ┆ [0.25923,        │\n",
       "│         ┆                  ┆                 ┆             ┆                  ┆ 0.184951, … 0.0] │\n",
       "└─────────┴──────────────────┴─────────────────┴─────────────┴──────────────────┴──────────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['ClassId', 'Hue_Hist', 'Saturation_Hist', 'Value_Hist', 'LBP_Hist', 'HOG_Features']\n",
    "features = train_sampled_df[columns]\n",
    "features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_hog = features['HOG_Features'].to_numpy()\n",
    "features_hog = np.stack(features_hog)\n",
    "features_hog.shape\n",
    "\n",
    "# split the data into train, validation and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_hog, features['ClassId'].to_numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize the data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lbp = features['LBP_Hist'].to_numpy()\n",
    "features_lbp = np.stack(features_lbp)\n",
    "features_lbp.shape\n",
    "\n",
    "# split the data into train, validation and test\n",
    "X_train_lbp, X_test_lbp, y_train_lbp, y_test_lbp = train_test_split(features_lbp, features['ClassId'].to_numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize the data\n",
    "X_train_lbp = X_train_lbp / 255\n",
    "X_test_lbp = X_test_lbp / 255\n",
    "\n",
    "features_hue = features['Hue_Hist'].to_numpy()\n",
    "features_hue = np.stack(features_hue)\n",
    "features_hue.shape\n",
    "\n",
    "# split the data into train, validation and test\n",
    "X_train_hue, X_test_hue, y_train_hue, y_test_hue = train_test_split(features_hue, features['ClassId'].to_numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize the data\n",
    "X_train_hue = X_train_hue / 255\n",
    "X_test_hue = X_test_hue / 255\n",
    "\n",
    "# create files for saturation\n",
    "features_sat = features['Saturation_Hist'].to_numpy()\n",
    "features_sat = np.stack(features_sat)\n",
    "\n",
    "# split the data into train, validation and test\n",
    "X_train_sat, X_test_sat, y_train_sat, y_test_sat = train_test_split(features_sat, features['ClassId'].to_numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize the data\n",
    "X_train_sat = X_train_sat / 255\n",
    "X_test_sat = X_test_sat / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2916"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 2916, 6)           132       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 2916, 6)           24        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 1458, 6)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1458, 16)          496       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 1458, 16)          64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 729, 16)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 11664)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               1399800   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 43)                3655      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1414335 (5.40 MB)\n",
      "Trainable params: 1414291 (5.40 MB)\n",
      "Non-trainable params: 44 (176.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ms_input_shape_hog = (X_train.shape[1], 1)\n",
    "ms_input_shape_lbp = (X_train_lbp.shape[1], 1)\n",
    "ms_input_shape_hue = (X_train_hue.shape[1], 1)\n",
    "ms_input_shape_sat = (X_train_sat.shape[1], 1)\n",
    "# define the model \n",
    "def create_cnn_model(padding='same',               \n",
    "                    input_shape= ms_input_shape_hog):\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=6, kernel_size=21, strides=1, padding=padding, activation='relu', \n",
    "                    input_shape= input_shape,kernel_initializer=keras.initializers.he_normal()))\n",
    "    model.add(BatchNormalization()) \n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding=padding))\n",
    "    model.add(Conv1D(filters=16, kernel_size=5, strides=1, padding=padding,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding=padding))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dense(84))\n",
    "    model.add(Dropout(rate=0.5, name='Dropout'))\n",
    "    model.add(Dense(43, activation='softmax', name='Output'))\n",
    "    return model\n",
    "\n",
    "model_hog = create_cnn_model(input_shape=ms_input_shape_hog)\n",
    "model_hog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 11s 54ms/step - loss: 1.2373 - acc: 0.6595 - val_loss: 4.4619 - val_acc: 0.0233\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 10s 58ms/step - loss: 0.2622 - acc: 0.9201 - val_loss: 5.1010 - val_acc: 0.0233\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 0.1129 - acc: 0.9642 - val_loss: 5.2937 - val_acc: 0.0807\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 9s 54ms/step - loss: 0.0622 - acc: 0.9818 - val_loss: 0.3542 - val_acc: 0.9077\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 11s 62ms/step - loss: 0.0433 - acc: 0.9873 - val_loss: 0.1893 - val_acc: 0.9448\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 10s 60ms/step - loss: 0.0395 - acc: 0.9889 - val_loss: 0.2413 - val_acc: 0.9440\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 10s 57ms/step - loss: 0.0338 - acc: 0.9880 - val_loss: 0.2110 - val_acc: 0.9506\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model_hog.compile(optimizer=tf.keras.optimizers.Adam(),loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "    ]\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 10\n",
    "\n",
    "history = model_hog.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "54/54 [==============================] - 1s 10ms/step - loss: 0.2488 - acc: 0.9436\n",
      "test loss, test acc: [0.24882948398590088, 0.9436046481132507]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_hog.evaluate(X_test, y_test, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 18, 6)             132       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 18, 6)             24        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 9, 6)              0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 9, 16)             496       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 9, 16)             64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 5, 16)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               9720      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 43)                3655      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24255 (94.75 KB)\n",
      "Trainable params: 24211 (94.57 KB)\n",
      "Non-trainable params: 44 (176.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#apply CNN to LBP features\n",
    "model_lbp = create_cnn_model(input_shape=ms_input_shape_lbp)\n",
    "model_lbp.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 2s 5ms/step - loss: 1.7344 - acc: 0.4689 - val_loss: 2.1679 - val_acc: 0.3656\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 1s 4ms/step - loss: 1.7056 - acc: 0.4766 - val_loss: 2.1904 - val_acc: 0.3605\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 1.6790 - acc: 0.4833 - val_loss: 2.2048 - val_acc: 0.3677\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 1.6775 - acc: 0.4875 - val_loss: 2.1563 - val_acc: 0.3663\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 1s 5ms/step - loss: 1.6984 - acc: 0.4797 - val_loss: 2.1598 - val_acc: 0.3714\n"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model_lbp.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "#train the model\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "    ]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "history_2 = model_lbp.fit(X_train_lbp,\n",
    "                    y_train_lbp,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 2.2671 - acc: 0.3593\n",
      "LBP test loss, LBP test acc: [2.267068862915039, 0.3593023121356964]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results_lbp = model_lbp.evaluate(X_test_lbp, y_test_lbp, batch_size=32)\n",
    "print(\"LBP test loss, LBP test acc:\", results_lbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 180, 6)            132       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 180, 6)            24        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 90, 6)             0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 90, 16)            496       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 90, 16)            64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 45, 16)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 720)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               86520     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 84)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 43)                3655      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101055 (394.75 KB)\n",
      "Trainable params: 101011 (394.57 KB)\n",
      "Non-trainable params: 44 (176.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "172/172 [==============================] - 3s 9ms/step - loss: 3.4416 - acc: 0.1121 - val_loss: 3.2528 - val_acc: 0.1541\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 2.8877 - acc: 0.1999 - val_loss: 2.7631 - val_acc: 0.2420\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 2.6033 - acc: 0.2562 - val_loss: 2.4351 - val_acc: 0.3212\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 2.3792 - acc: 0.3129 - val_loss: 2.2501 - val_acc: 0.3532\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 2.2171 - acc: 0.3561 - val_loss: 2.1238 - val_acc: 0.3946\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 2.0624 - acc: 0.4046 - val_loss: 1.9900 - val_acc: 0.4331\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.9129 - acc: 0.4433 - val_loss: 1.9492 - val_acc: 0.4411\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.7905 - acc: 0.4724 - val_loss: 1.8850 - val_acc: 0.4695\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.6916 - acc: 0.4956 - val_loss: 1.7907 - val_acc: 0.4920\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.5699 - acc: 0.5298 - val_loss: 1.7970 - val_acc: 0.4978\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.4884 - acc: 0.5505 - val_loss: 1.7581 - val_acc: 0.5051\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.4078 - acc: 0.5756 - val_loss: 1.7783 - val_acc: 0.5109\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.3209 - acc: 0.5918 - val_loss: 1.8462 - val_acc: 0.4985\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.2617 - acc: 0.6185 - val_loss: 1.8051 - val_acc: 0.5109\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.1996 - acc: 0.6310 - val_loss: 1.7628 - val_acc: 0.5138\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.0976 - acc: 0.6606 - val_loss: 1.8314 - val_acc: 0.5211\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 1.0650 - acc: 0.6632 - val_loss: 1.7746 - val_acc: 0.5189\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.9795 - acc: 0.7004 - val_loss: 1.8503 - val_acc: 0.5254\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.9471 - acc: 0.7060 - val_loss: 1.7960 - val_acc: 0.5109\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.8892 - acc: 0.7253 - val_loss: 1.9155 - val_acc: 0.5298\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.8480 - acc: 0.7387 - val_loss: 1.8623 - val_acc: 0.5262\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.8122 - acc: 0.7495 - val_loss: 1.9187 - val_acc: 0.5400\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 1s 7ms/step - loss: 0.7449 - acc: 0.7640 - val_loss: 1.8692 - val_acc: 0.5501\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.7210 - acc: 0.7751 - val_loss: 2.0116 - val_acc: 0.5153\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.6696 - acc: 0.7851 - val_loss: 2.0896 - val_acc: 0.5189\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.6371 - acc: 0.7991 - val_loss: 2.0649 - val_acc: 0.5269\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 1s 8ms/step - loss: 0.6476 - acc: 0.7943 - val_loss: 2.0772 - val_acc: 0.5291\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model_hue = create_cnn_model(input_shape=ms_input_shape_hue)\n",
    "model_hue.summary()\n",
    "\n",
    "model_hue.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "#train the model\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "    ]   \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "history_3 = model_hue.fit(X_train_hue,\n",
    "                    y_train_hue,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 1.8207 - acc: 0.5657\n",
      "HUE test loss, HUE test acc: [1.8206799030303955, 0.5656976699829102]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results_hue = model_hue.evaluate(X_test_hue, y_test_hue, batch_size=32)\n",
    "print(\"HUE test loss, HUE test acc:\", results_hue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 3s 10ms/step - loss: 3.6555 - acc: 0.0814 - val_loss: 3.4584 - val_acc: 0.0850\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 3.2396 - acc: 0.1344 - val_loss: 3.0988 - val_acc: 0.1679\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 2.9814 - acc: 0.1846 - val_loss: 2.8625 - val_acc: 0.2166\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 2s 10ms/step - loss: 2.7734 - acc: 0.2342 - val_loss: 2.6388 - val_acc: 0.2965\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 2.5798 - acc: 0.2825 - val_loss: 2.5426 - val_acc: 0.3190\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 2.4050 - acc: 0.3214 - val_loss: 2.5504 - val_acc: 0.3089\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 2.2512 - acc: 0.3630 - val_loss: 2.3857 - val_acc: 0.3445\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 2s 10ms/step - loss: 2.0823 - acc: 0.4108 - val_loss: 2.3628 - val_acc: 0.3677\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 2s 10ms/step - loss: 1.9178 - acc: 0.4504 - val_loss: 2.3722 - val_acc: 0.3568\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.7954 - acc: 0.4822 - val_loss: 2.3243 - val_acc: 0.3859\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 2s 10ms/step - loss: 1.6516 - acc: 0.5176 - val_loss: 2.4107 - val_acc: 0.3866\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.5541 - acc: 0.5516 - val_loss: 2.3900 - val_acc: 0.3874\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.4262 - acc: 0.5781 - val_loss: 2.4115 - val_acc: 0.3990\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.3057 - acc: 0.6085 - val_loss: 2.4448 - val_acc: 0.3939\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.2250 - acc: 0.6424 - val_loss: 2.5540 - val_acc: 0.3837\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.1283 - acc: 0.6710 - val_loss: 2.6679 - val_acc: 0.3837\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 1.0385 - acc: 0.6873 - val_loss: 2.7594 - val_acc: 0.3757\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.9410 - acc: 0.7149 - val_loss: 2.9200 - val_acc: 0.3648\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.8607 - acc: 0.7349 - val_loss: 2.9405 - val_acc: 0.3706\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 1s 9ms/step - loss: 0.7918 - acc: 0.7589 - val_loss: 3.0873 - val_acc: 0.3634\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.6923 - acc: 0.7852 - val_loss: 3.1383 - val_acc: 0.3626\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 2s 10ms/step - loss: 0.6823 - acc: 0.7916 - val_loss: 3.2610 - val_acc: 0.3663\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.6471 - acc: 0.8056 - val_loss: 3.3816 - val_acc: 0.3641\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.5923 - acc: 0.8165 - val_loss: 3.3557 - val_acc: 0.3714\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.5535 - acc: 0.8265 - val_loss: 3.4494 - val_acc: 0.3670\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.4958 - acc: 0.8459 - val_loss: 3.9849 - val_acc: 0.3605\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.4877 - acc: 0.8467 - val_loss: 3.9204 - val_acc: 0.3692\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.4507 - acc: 0.8619 - val_loss: 3.7126 - val_acc: 0.3547\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.4060 - acc: 0.8754 - val_loss: 4.3882 - val_acc: 0.3576\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 2s 9ms/step - loss: 0.4007 - acc: 0.8739 - val_loss: 3.9114 - val_acc: 0.3721\n",
      "Evaluate on test data\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 3.9677 - acc: 0.3459\n",
      "SAT test loss, SAT test acc: [3.967654228210449, 0.34593021869659424]\n"
     ]
    }
   ],
   "source": [
    "# create model for saturation\n",
    "model_sat = create_cnn_model(input_shape=ms_input_shape_sat)\n",
    "\n",
    "model_sat.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "#train the model\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "    ]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "history_4 = model_sat.fit(X_train_sat,\n",
    "                    y_train_sat,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "                \n",
    "# evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results_sat = model_sat.evaluate(X_test_sat, y_test_sat, batch_size=32)\n",
    "print(\"SAT test loss, SAT test acc:\", results_sat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "281-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
